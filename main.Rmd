---
title: "ST 558 Project 3 Analysis"
author: "Carter Hall"
date: "2023-11-05"
output: github_document
params:
  Education: "Middle or Less"
---

```{r include=FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(caret)
library(cowplot)
library(gridExtra)
library(caret)
set.seed(558)
```

```{r rendering, eval=FALSE}
#' Include rmarkdown::render code from lecture slides to iterate through the 
#' levels of education (supplied below). 
#' 
#' This might have to be moved to a different file! :)
for (educ_level in c("Middle or Less", "Some High", "High School Graduate",
                     "Some College", "College_Graduate")) {
rmarkdown::render("./main.Rmd", output_file = paste0(tolower(gsub(" ", "_",
                                                                  x = educ_level)
                                                             ),
                                                     ".html"
                                                     ),
                  params = list(Education = educ_level)
                  )
}
```


# Introduction

The following analysis uses a subset of data from the 2015 Behavioral Risk Factor Surveillance System (BRFSS). The dataset consists of three files with survey responses pertaining to diabetes.
# Data

```{r}
# Importing the Data
diab <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

#' Context on the `Education` variable:
#' 1 := Never attended school or at most kindergarten
#' 2 := Grades 1-8   [1 and 2 Renamed to Group 1]
#' 3 := Grades 9-11          [Renamed to Group 2]
#' 4 := Grade 12 or GED      [Renamed to Group 3]
#' 5 := 1-3 Years of College [Renamed to Group 4]
#' 6 := 4+ Years of College  [Renamed to Group 5]

# Preprocessing Education Groups by Combining 1 and 2
diab <- diab %>% mutate(
  Education = case_when(
    Education %in% c(1,2) ~ "Middle or Less",
    Education == 3 ~ "Some High",
    Education == 4 ~ "High School Graduate",
    Education == 5 ~ "Some College",
    Education == 6 ~ "College Graduate"
  ),
  Sex = factor(x = Sex, levels = c(0,1), labels = c("Female", "Male")),
  DiffWalk = factor(x = DiffWalk, levels = c(0,1), labels = c("No", "Yes")),
  GenHlth = factor(x = GenHlth, levels = 1:5,
                   labels = c("Excellent", "Very Good", "Good", "Fair", "Poor")),
  NoDocbcCost = factor(NoDocbcCost, levels = c(0,1), labels = c("No", "Yes")),
  AnyHealthcare = factor(AnyHealthcare, levels = c(0,1), labels = c("No", "Yes")),
  HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0,1), labels = c("No", "Yes")),
  HighBP = factor(HighBP, levels = c(0,1), labels = c("No", "Yes")),
  HighChol = factor(HighChol, levels = c(0,1), labels = c("No", "Yes")),
  CholCheck = factor(CholCheck, levels = c(0,1), labels = c("No", "Yes")),
  Smoker = factor(Smoker, levels = c(0,1), labels = c("No", "Yes")),
  Stroke = factor(Stroke, levels = c(0,1), labels = c("No", "Yes")),
  HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0,1), labels = c("No", "Yes")),
  PhysActivity = factor(PhysActivity,levels = c(0,1), labels = c("No", "Yes")),
  Fruits = factor(Fruits, levels = c(0,1), labels = c("No", "Yes")),
  Veggies = factor(Veggies, levels = c(0,1), labels = c("No", "Yes")),
  Age = factor(Age, levels = c(1:14),
               labels = c("18-24","25-29","30-34","35-39","40-44","45-49","50-54",
                          "55-59","60-64","65-69","70-74","75-79","80+",NA)),
  Income = factor(Income, levels = c(1:9)),
  PhysHlth = factor(Income, levels = c(1:30)),
  MentHlth = factor(MentHlth, levels = c(1:30))
)

diab_subset <- diab %>% filter(Education == params$Education)
```


# Summarizations

In determining the evaluation of potentially significant variables for predicting a response, the first phase is often getting an understanding for the data. This section focused on **Exploratory Data Analysis** will offer some automated graphs that describe the relationship between variables of-interest while offering insight into how statisticians and audiences outside of statistics might read the graph to glean information.

## Univariate Visualizations of Categorical Variables

As the preprocessing for this dataset included the recasting of much of the raw data into *factors*, we might be interested in the relative frequencies of many of the levels of our categorical variables. From the many, many graphs below, we can obtain some summary information about the shape of our variables' distributions and the similarities and differences between the proportion of respondents who have Diabetes and the proportions of respondents who have a certain income (see the `Income` variable) or certain food intake (see the `Fruits` and `Veggies` Variables).

```{r}
cat_summ <- diab_subset %>% select(where(is.factor)) %>% 
  pivot_longer(cols = colnames(.)) %>%
  table() %>% as.data.frame(.) %>% mutate(
    isValidLevel = apply(., 1, function(x) {
  return( as.list(x)$value %in% unique(diab_subset[[as.list(x)$name]]) )
  })
  ) %>% filter(isValidLevel) %>% select(-isValidLevel) 

cat_plots <- lapply(unique(cat_summ$name), 
                    function(x) {
                      temp <- cat_summ %>% filter(name == x)
                      return(
                        ggplot(data = temp,
                               aes(x = value, y = Freq)) + 
                          geom_bar(
                            stat = "identity"
                        ) + labs(title = 
                                   paste0(temp$name),
                                 subtitle = paste0("N = ", nrow(temp)),
                                 y = "Count",
                                 x = "Response"
                                 ) +
                          theme_bw()
                      )
                    })
```

```{r}
grid.arrange(grobs = cat_plots[1:4], nCol = 2)
grid.arrange(grobs = cat_plots[5:8], nCol = 2)
grid.arrange(grobs = cat_plots[9:12], nCol = 2)
grid.arrange(grobs = cat_plots[13:16], nCol = 2)
grid.arrange(grobs = cat_plots[17:19], nCol = 2)
```

## Summarization 2

## Summarization 3

# Modeling

## Reproducible Train/Test Split

Consider the below code chunk to partition the data into *train* and *test* datasets
based on a 70-30 split.

```{r}
# 70% <=> p = 0.7
indices <- createDataPartition(1:nrow(diab_subset), p = 0.7,list = F) 
train   <- diab_subset[indices,]
test    <- diab_subset[-indices,]
```

## Discussion of Candidate Models and Related Items

### Log Loss vs Accuracy

Consider a vector $\pmb{Y} = (y_1,...,y_n)$ of $n$ true labels/classes for $n$ data points in a testing set, and consider a classification model attempting to identify data as belonging to a particular class. Because the task of classification is a *supervised* one, wherein we supply *labeled* features to a model, it stands to reason that we desire a model that classifies well on both training data and unseen data. This idea, referred to as **generalization**, implies that a model could be trained to near-perfection on supplied observations but fail miserably when applied in new scenarios! 

To aid this process of generalization, statisticians introduce the concept of *loss* into the model-training process; in essence, loss asks the question
> How far off am I, the model, from ground truth?

To mathematically *quantify* loss and *minimize* it is as much of an art as it is pure mathematics -- consider questions such as 

- **Are all misclassifications treated equally?** (Consider a false negative diagnosis to a cancer patient, versus a false positive for a light bulb factory.)

- **How should a model be penalized, or have its parameters adjusted, for a misclassification?** (This question lends itself to a discussion of algorithms such as **gradient descent** and **stochastic gradient descent**, wherein we generally want to avoid reaching a local minima of a loss function for which there are sets of [hyper-]parameters that yield better classification and generalization. Consider also the regression analog of this question yielded methods such as the LASSO (Tibshirani, 1996).)

For this discussion, we refer to loss functions $\mathcal{L}(y_i, p_i)$ as being **non-negative definite**, i.e., $\mathcal{L}(y_i, p_i)\geq 0 \forall y_i$. 

Consider two of the more simpler functions used to evaluate classification models, **log loss** (also known as cross-entropy loss) and **accuracy**. In a binary setting, we define the log loss obtained from predicting an observation to belong to the "positive" class with probability $p_i \in (0,1)$ and true label $y_i \in \{0,1\}$ as 
$$ \mathcal{L}(y_i, p_i) = -[y_i \ln(p_i) + (1-y_i)\ln(1-p_i)] $$
If we adopted similar notation, the **accuracy** loss function, defined with parameters $y_i \in \{0,\1}$ as the true label and $\hat{y}_i \in \{0,1\}$ as the predicted label, wherein we might define 
$$ \hat{y}_i = \begin{cases} 1 & p_i > c \\ 0 & p_i \leq c \end{cases} $$ 
such that we declare the model as predicting the observation belonging to the positive class iff the probability at which it does so exceeds some threshold $c \in [0,1]$ (i.e., $c = 0.5$). We then define accuracy for the $i$-th observation as
$$ \mathcal{L}(y_i, \hat{y_i}) = \mathbb{I}(y_i = \hat{y}_i) $$
with $\mathbb{I}$ the indicator function, such that
$$ \text{Accuracy} = \frac{1}{N}\sum_{i=1}^{N} \mathcal{L}(y_i, \hat{y}_i) $$ 

The question, naturally, is 
> What's the difference between log-loss and accuracy?

The preference of the former lies in the harsher *penalization* by the accuracy metric in incorrect cases. For example, consider a case where the true label is $1$, a positive classification; even if the model emits a positive classification with probability $p_i = c - \epsilon$ for some small $\epsilon > 0$, the accuracy metric regards this as incorrect, no matter how close the model was to the truth! Because the log loss function is differentiable, we can iteratively update the hyperparameters of a classification model in the training process, applying more drastic adjustments when the model predicts the incorrect class with greater certainty. 

In a more cross-validation perspective, wherein we might supply a discretized grid of tuning parameters, we can better understand the landscape of our multi-dimensional loss function by identifying which parameter adjustments elicit the largest changes in log-loss! An accuracy metric, as mentioned earlier, may fail to capture the more fine discrepancies between sets of parameters. (Consider the multi-class loss function below, where we now define $y_i \in \{0,1\}$ as representing class $i \in \{1,2,...,n\}$ to be the true class.)

$$ \mathcal{L}(y_i,p_i) = -\sum_{i=1}^{n} y_i \ln(p_i) $$

**This** ability to adjust parameters and view the effects of these adjustments is why log-loss is preferred to accuracy in many machine learning applications as a means of evaluating the performance of classification models!  

### Logistic Regression Models

#### Description
For Logistic Regression, consider labels $Y \in \{0,1\}$, where these values encode "failure/success" in a binary sense. Predictions $\hat{Y} \in \{0,1\}$ by definition, and so we might model success probability via
$$ P(Y|X) = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}} $$
where the RHS is the **logistic function**. The **link function** in logistic regression is given by
$$ g(\mu) = \beta_0 + \beta_1 x_1 + ... + \beta_p x_p  $$ 
where $\mu$ is the mean for the set of $x$-values used. (Note $g(\mu) = \mu$ for standard linear regression, which is called the *identity link* s.t. the value predicted is mean response given $X$.)

However, there is no closed form solution used to fit $\beta_0,\beta_1$, and so MLE often used to fit parameter. What instead we might do is write the **logit function** or **log-odds function**
$$ \ln\Big(\frac{P(Y|X)}{1-P(Y|X)}\Big) = \beta_0 + \beta_1 X  $$
which is linear in the parameters and offers a more intuitive explanation and interpretation of coefficients (e.g., $\beta_1$ is now a change in log-odds of success).

In a *multi-class* scenario, wherein $Y$ takes on more than two values, and when $\pmb{X} = (1, x_1,...,x_p)$ is more than one predictor, consider that
$$ P(Y|\pmb{X}) = \frac{e^{\beta_0 + \beta_1 x_1 + ... + \beta_p x_p}}{1 + e^{\beta_0 + \beta_1 x_1 + ... + \beta_p x_p}} = \frac{e^{\pmb{\beta}\pmb{X}}}{1 + e^{\pmb{\beta}\pmb{X}}} $$ 
and the logit function then becomes
$$ \ln\Big(\frac{P(Y|\pmb{X})}{1-P(Y|\pmb{X})}\Big) = \pmb{\beta}\pmb{X} $$
with $\pmb{\beta} = \begin{bmatrix} \beta_0 & \beta_1 & ... & \beta_p \end{bmatrix}$.

#### Why Use in Classification?

With this type of data, we consider logistic regression as a candidate modeling procedure because of a multitude of reasons, including

1. **Ability to Handle Nonlinear Relationships**: Logistic regression is built on the idea that there could be a nonlinear relationship between the class and features; feature engineering can also easily be incorporated when parameterizing a logistic regression model with candidate predictors.

2. **Ease of Interpretation**: The interpreted values from a logistic regression model are by nature the *probability of that observation belong to a positive class* (binary) *or to that particular class* (multi-class problem, wherein each model predicts for a single class and the predictions for each individual model are aggregated via maximums to obtain a single prediction for the observation).

#### Fitting of Candidate Logistic Regression Models

```{r}

```


### LASSO Regression Model
The lasso performs variable selection. It yields *sparse* models that only involve a subset of the variables. The lasso is more resistant to outliers than linear regression. The purpose of the lasso model is to prevent overfitting.
```{r}

control <- trainControl(method="cv", number=5, classProbs=TRUE, summaryFunction=mnLogLoss)

set.seed(100)

lassoFit <- train(Diabetes_binary ~ .,
                     data = train,
                     method = "glmnet",
                     preProcess = c("center", "scale"),
                     metric = "logLoss",
                     trControl =  control)
```

### Classification Tree Model

A classification tree is a tree-based method that splits the data into groups or regions. 
```{r}
control <- trainControl(method="cv", number=5, classProbs=TRUE, summaryFunction=mnLogLoss)
#set seed before training
set.seed(50)

classTreeFit <-train(Diabetes_binary ~ .,
                     data = train,
                     method = "rpart",
                     preProcess = c("center", "scale"),
                     trControl =  control)

```


### Random Forest Model

### "New" Models

#### New Model 1: 

#### New Model 2:

# Final Model Selection