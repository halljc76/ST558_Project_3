---
title: "ST 558 Project 3 Analysis"
author: "Carter Hall"
date: "2023-11-05"
output: github_document
params:
  Education: "Middle or Less"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(caret)
```

```{r rendering, include=FALSE}
#' Include rmarkdown::render code from lecture slides to iterate through the 
#' levels of education (supplied below). 
#' 
#' Not sure if the params is needed in the YAML header a priori to running
#' code in this chunk, but it may not *hurt*, especially as a way to check!
#' 
#' Also not sure if this for loop will work :p
for (educ_level in c("Middle or Less", "Some High", "High School Graduate",
                     "Some College", "College_Graduate")) {
rmarkdown::render("./main.Rmd", output_file = paste0(tolower(gsub(" ", "_",
                                                                  x = educ_level)
                                                             ),
                                                     ".html"
                                                     ),
                  params = list(Education = educ_level)
                  )
}
```


# Introduction

The following analysis uses a subset of data from the 2015 Behavioral Risk Factor Surveillance System (BRFSS). The dataset consists of three files with survey responses pertaining to diabetes.
# Data

```{r}
# Importing the Data
diab <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

#' Context on the `Education` variable:
#' 1 := Never attended school or at most kindergarten
#' 2 := Grades 1-8   [1 and 2 Renamed to Group 1]
#' 3 := Grades 9-11          [Renamed to Group 2]
#' 4 := Grade 12 or GED      [Renamed to Group 3]
#' 5 := 1-3 Years of College [Renamed to Group 4]
#' 6 := 4+ Years of College  [Renamed to Group 5]

# Preprocessing Education Groups by Combining 1 and 2
diab <- diab %>% mutate(
  Education = case_when(
    Education %in% c(1,2) ~ "Middle or Less",
    Education == 3 ~ "Some High",
    Education == 4 ~ "High School Graduate",
    Education == 5 ~ "Some College",
    Education == 6 ~ "College Graduate"
  ),
  Sex = factor(x = Sex, levels = c(0,1), labels = c("Female", "Male")),
  DiffWalk = factor(x = DiffWalk, levels = c(0,1), labels = c("No", "Yes")),
  GenHlth = factor(x = GenHlth, levels = 1:5,
                   labels = c("Excellent", "Very Good", "Good", "Fair", "Poor")),
  NoDocbcCost = factor(NoDocbcCost, levels = c(0,1), labels = c("No", "Yes")),
  AnyHealthcare = factor(AnyHealthcare, levels = c(0,1), labels = c("No", "Yes")),
  HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0,1), labels = c("No", "Yes")),
  HighBP = factor(HighBP, levels = c(0,1), labels = c("No", "Yes")),
  HighChol = factor(HighChol, levels = c(0,1), labels = c("No", "Yes")),
  CholCheck = factor(CholCheck, levels = c(0,1), labels = c("No", "Yes")),
  Smoker = factor(Smoker, levels = c(0,1), labels = c("No", "Yes")),
  Stroke = factor(Stroke, levels = c(0,1), labels = c("No", "Yes")),
  HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0,1), labels = c("No", "Yes")),
  PhysActivity = factor(PhysActivity,levels = c(0,1), labels = c("No", "Yes")),
  Fruits = factor(Fruits, levels = c(0,1), labels = c("No", "Yes")),
  Veggies = factor(Veggies, levels = c(0,1), labels = c("No", "Yes")),
  Age = factor(Age, levels = c(1:14),
               labels = c("18-24","25-29","30-34","35-39","40-44","45-49","50-54",
                          "55-59","60-64","65-69","70-74","75-79","80+",NA))
)

diab_subset <- diab %>% filter(Education == params$Education)
```


# Summarizations

# Modeling

## Reproducible Train/Test Split

Consider the below code chunk to partition the data into *train* and *test* datasets
based on a 70-30 split.

```{r}
# 70% <=> p = 0.7
indices <- createDataPartition(1:nrow(diab_subset), p = 0.7,list = F) 
train   <- diab_subset[indices,]
test    <- diab_subset[-indices,]
```

## Discussion of Candidate Models and Related Items

### Log Loss vs Accuracy

Consider a vector $\pmb{Y} = (y_1,...,y_n)$ of $n$ true labels/classes for $n$ data points in a testing set, and consider a classification model attempting to identify data as belonging to a particular class. Because the task of classification is a *supervised* one, wherein we supply *labeled* features to a model, it stands to reason that we desire a model that classifies well on both training data and unseen data. This idea, referred to as **generalization**, implies that a model could be trained to near-perfection on supplied observations but fail miserably when applied in new scenarios! 

To aid this process of generalization, statisticians introduce the concept of *loss* into the model-training process; in essence, loss asks the question
> How far off am I, the model, from ground truth?

To mathematically *quantify* loss and *minimize* it is as much of an art as it is pure mathematics -- consider questions such as 

- **Are all misclassifications treated equally?** (Consider a false negative diagnosis to a cancer patient, versus a false positive for a light bulb factory.)

- **How should a model be penalized, or have its parameters adjusted, for a misclassification?** (This question lends itself to a discussion of algorithms such as **gradient descent** and **stochastic gradient descent**, wherein we generally want to avoid reaching a local minima of a loss function for which there are sets of [hyper-]parameters that yield better classification and generalization. Consider also the regression analog of this question yielded methods such as the LASSO (Tibshirani, 1996).)

For this discussion, we refer to loss functions $\mathcal{L}(y_i, p_i)$ as being **non-negative definite**, i.e., $\mathcal{L}(y_i, p_i)\geq 0 \forall y_i$. 

Consider two of the more simpler functions used to evaluate classification models, **log loss** (also known as cross-entropy loss) and **accuracy**. In a binary setting, we define the log loss obtained from predicting an observation to belong to the "positive" class with probability $p_i \in (0,1)$ and true label $y_i \in \{0,1\}$ as 
$$ \mathcal{L}(y_i, p_i) = -[y_i \ln(p_i) + (1-y_i)\ln(1-p_i)] $$
If we adopted similar notation, the **accuracy** loss function, defined with parameters $y_i \in \{0,\1}$ as the true label and $\hat{y}_i \in \{0,1\}$ as the predicted label, wherein we might define 
$$ \hat{y}_i = \begin{cases} 1 & p_i > c \\ 0 & p_i \leq c \end{cases} $$ 
such that we declare the model as predicting the observation belonging to the positive class iff the probability at which it does so exceeds some threshold $c \in [0,1]$ (i.e., $c = 0.5$). We then define accuracy for the $i$-th observation as
$$ \mathcal{L}(y_i, \hat{y_i}) = \mathbb{I}(y_i = \hat{y}_i) $$
with $\mathbb{I}$ the indicator function, such that
$$ \text{Accuracy} = \frac{1}{N}\sum_{i=1}^{N} \mathcal{L}(y_i, \hat{y}_i) $$ 

The question, naturally, is 
> What's the difference between log-loss and accuracy?

The preference of the former lies in the harsher *penalization* by the accuracy metric in incorrect cases. For example, consider a case where the true label is $1$, a positive classification; even if the model emits a positive classification with probability $p_i = c - \epsilon$ for some small $\epsilon > 0$, the accuracy metric regards this as incorrect, no matter how close the model was to the truth! Because the log loss function is differentiable, we can iteratively update the hyperparameters of a classification model in the training process, applying more drastic adjustments when the model predicts the incorrect class with greater certainty. 

In a more cross-validation perspective, wherein we might supply a discretized grid of tuning parameters, we can better understand the landscape of our multi-dimensional loss function by identifying which parameter adjustments elicit the largest changes in log-loss! An accuracy metric, as mentioned earlier, may fail to capture the more fine discrepancies between sets of parameters. (Consider the multi-class loss function below, where we now define $y_i \in \{0,1\}$ as representing class $i \in \{1,2,...,n\}$ to be the true class.)

$$ \mathcal{L}(y_i,p_i) = -\sum_{i=1}^{n} y_i \ln(p_i) $$

**This** ability to adjust parameters and view the effects of these adjustments is why log-loss is preferred to accuracy in many machine learning applications as a means of evaluating the performance of classification models!  

### Logistic Regression Models

### LASSO Regression Model

### Classification Tree Model

### Random Forest Model

### "New" Models

#### New Model 1: 

#### New Model 2:

# Final Model Selection